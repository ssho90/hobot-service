"""
TradingEconomics ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (DB ì—†ì´ ì‹¤í–‰)
"""
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

import re
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsCollectorTester:
    """DB ì—†ì´ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    BASE_URL = "https://tradingeconomics.com"
    STREAM_URL = "https://tradingeconomics.com/stream"
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.timeout = 30
    
    def fetch_stream_page(self, use_selenium: bool = False) -> Optional[str]:
        """TradingEconomics ìŠ¤íŠ¸ë¦¼ í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
        if use_selenium:
            return self._fetch_with_selenium()
        else:
            return self._fetch_with_requests()
    
    def _fetch_with_requests(self) -> Optional[str]:
        """requestsë¥¼ ì‚¬ìš©í•˜ì—¬ HTML ê°€ì ¸ì˜¤ê¸°"""
        try:
            logger.info(f"í˜ì´ì§€ ìš”ì²­: {self.STREAM_URL}")
            response = self.session.get(self.STREAM_URL, timeout=self.timeout)
            response.raise_for_status()
            logger.info(f"ì‘ë‹µ ì„±ê³µ (ìƒíƒœ ì½”ë“œ: {response.status_code})")
            html = response.text
            
            # ë””ë²„ê¹…: HTMLì„ íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒì )
            save_html = input("\nHTMLì„ íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
            if save_html:
                html_file = "tradingeconomics_stream.html"
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(html)
                print(f"âœ… HTMLì´ {html_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                print(f"   íŒŒì¼ í¬ê¸°: {len(html)} bytes")
                print(f"   'stream' ë¬¸ìì—´ í¬í•¨ ì—¬ë¶€: {'stream' in html}")
                print(f"   'te-stream-item' ë¬¸ìì—´ í¬í•¨ ì—¬ë¶€: {'te-stream-item' in html}")
            
            return html
        except requests.exceptions.RequestException as e:
            logger.error(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
    
    def _fetch_with_selenium(self) -> Optional[str]:
        """Seleniumì„ ì‚¬ìš©í•˜ì—¬ JavaScript ë Œë”ë§ëœ HTML ê°€ì ¸ì˜¤ê¸°"""
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.common.exceptions import TimeoutException
        except ImportError:
            print("âŒ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì„¤ì¹˜: pip install selenium")
            print("   ChromeDriverë„ í•„ìš”í•©ë‹ˆë‹¤: https://chromedriver.chromium.org/")
            return None
        
        driver = None
        try:
            print("ğŸŒ Seleniumì„ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ë¡œë“œ ì¤‘...")
            
            # Chrome ì˜µì…˜ ì„¤ì •
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            driver = webdriver.Chrome(options=chrome_options)
            driver.get(self.STREAM_URL)
            
            # stream divê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
            try:
                WebDriverWait(driver, 30).until(
                    EC.presence_of_element_located((By.ID, "stream"))
                )
                print("âœ… stream div ë¡œë“œ ì™„ë£Œ")
                
                # ì¶”ê°€ë¡œ ë‰´ìŠ¤ í•­ëª©ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                WebDriverWait(driver, 10).until(
                    lambda d: len(d.find_elements(By.CSS_SELECTOR, "li.te-stream-item")) > 0
                )
                print(f"âœ… ë‰´ìŠ¤ í•­ëª© ë¡œë“œ ì™„ë£Œ ({len(driver.find_elements(By.CSS_SELECTOR, 'li.te-stream-item'))}ê°œ)")
            except TimeoutException:
                print("âš ï¸  ë‰´ìŠ¤ í•­ëª© ë¡œë“œ íƒ€ì„ì•„ì›ƒ (ì¼ë¶€ë§Œ ë¡œë“œë˜ì—ˆì„ ìˆ˜ ìˆìŒ)")
            
            html = driver.page_source
            
            # ë””ë²„ê¹…: HTMLì„ íŒŒì¼ë¡œ ì €ì¥
            save_html = input("\nHTMLì„ íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
            if save_html:
                html_file = "tradingeconomics_stream_selenium.html"
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(html)
                print(f"âœ… HTMLì´ {html_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                print(f"   íŒŒì¼ í¬ê¸°: {len(html)} bytes")
            
            return html
            
        except Exception as e:
            print(f"âŒ Selenium ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None
        finally:
            if driver:
                driver.quit()
    
    def parse_news_items(self, html: str) -> List[Dict]:
        """HTMLì—ì„œ ë‰´ìŠ¤ í•­ëª© íŒŒì‹±"""
        soup = BeautifulSoup(html, 'html.parser')
        news_items = []
        
        # ë°©ë²• 1: div#stream ì•ˆì—ì„œ ì°¾ê¸°
        stream_div = soup.find('div', id='stream')
        if stream_div:
            print("âœ… div#streamì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            # stream div ì•ˆì—ì„œ li ìš”ì†Œ ì°¾ê¸°
            stream_items = stream_div.find_all('li', class_=re.compile(r'te-stream-item', re.I))
            print(f"   div#stream ì•ˆì—ì„œ {len(stream_items)}ê°œì˜ li ìš”ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸  div#streamì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            stream_items = []
        
        # ë°©ë²• 2: stream divê°€ ì—†ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì „ì²´ ë¬¸ì„œì—ì„œ ì°¾ê¸°
        if not stream_items:
            print("ì „ì²´ ë¬¸ì„œì—ì„œ te-stream-item í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.")
            stream_items = soup.find_all('li', class_=re.compile(r'te-stream-item', re.I))
            print(f"   ì „ì²´ ë¬¸ì„œì—ì„œ {len(stream_items)}ê°œì˜ li ìš”ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # ë°©ë²• 3: í´ë˜ìŠ¤ ì´ë¦„ì„ ì •í™•íˆ ë§¤ì¹­
        if not stream_items:
            print("ì •í™•í•œ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.")
            stream_items = soup.find_all('li', class_=lambda x: x and 'te-stream-item' in ' '.join(x))
            print(f"   ì •í™•í•œ í´ë˜ìŠ¤ ë§¤ì¹­ìœ¼ë¡œ {len(stream_items)}ê°œì˜ li ìš”ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # ë°©ë²• 4: list-group-itemê³¼ í•¨ê»˜ ì°¾ê¸°
        if not stream_items:
            print("list-group-item í´ë˜ìŠ¤ë¥¼ ê°€ì§„ li ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.")
            stream_items = soup.find_all('li', class_=lambda x: x and 'list-group-item' in ' '.join(x))
            print(f"   list-group-itemìœ¼ë¡œ {len(stream_items)}ê°œì˜ li ìš”ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        logger.info(f"íŒŒì‹±ëœ ë‰´ìŠ¤ í•­ëª© ìˆ˜: {len(stream_items)}")
        
        # ë””ë²„ê¹…: ì²« ë²ˆì§¸ í•­ëª©ì˜ HTML êµ¬ì¡° í™•ì¸
        if stream_items and len(stream_items) > 0:
            first_item = stream_items[0]
            print(f"\nğŸ” ì²« ë²ˆì§¸ í•­ëª© ë¶„ì„:")
            print(f"   í´ë˜ìŠ¤: {first_item.get('class', [])}")
            print(f"   ID: {first_item.get('id', 'N/A')}")
            print(f"   HTML (ì²˜ìŒ 500ì):\n{str(first_item)[:500]}\n")
        else:
            print("\nâš ï¸  ë‰´ìŠ¤ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤...")
            # stream divê°€ ìˆëŠ”ì§€ í™•ì¸
            if stream_div:
                print(f"   stream divì˜ ìì‹ ìš”ì†Œ ìˆ˜: {len(list(stream_div.children))}")
                print(f"   stream divì˜ ì§ì ‘ ìì‹ li ìˆ˜: {len(stream_div.find_all('li', recursive=False))}")
                print(f"   stream divì˜ ëª¨ë“  li ìˆ˜: {len(stream_div.find_all('li'))}")
        
        for item in stream_items:
            try:
                news_item = self._extract_news_item(item)
                if news_item:
                    news_items.append(news_item)
                else:
                    print(f"   âš ï¸  ë‰´ìŠ¤ í•­ëª© ì¶”ì¶œ ì‹¤íŒ¨ (ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ)")
            except Exception as e:
                logger.warning(f"ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        return news_items
    
    def _extract_news_item(self, element) -> Optional[Dict]:
        """ê°œë³„ ë‰´ìŠ¤ í•­ëª©ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì œëª© ì¶”ì¶œ: <a class="te-stream-title-2">
            title_elem = element.find('a', class_=re.compile(r'te-stream-title', re.I))
            if not title_elem:
                # ë””ë²„ê¹…: ì™œ ì œëª©ì„ ì°¾ì§€ ëª»í–ˆëŠ”ì§€ í™•ì¸
                all_links = element.find_all('a')
                print(f"   âš ï¸  ì œëª© ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì „ì²´ ë§í¬ ìˆ˜: {len(all_links)}")
                if all_links:
                    print(f"   ì²« ë²ˆì§¸ ë§í¬ í´ë˜ìŠ¤: {all_links[0].get('class', [])}")
                return None
            
            # <b> íƒœê·¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ë˜ëŠ” ì§ì ‘ í…ìŠ¤íŠ¸
            title_b = title_elem.find('b')
            if title_b:
                title = title_b.get_text(strip=True)
            else:
                title = title_elem.get_text(strip=True)
            
            if not title:
                return None
            
            # ë§í¬ ì¶”ì¶œ
            link = title_elem.get('href', '')
            if link and not link.startswith('http'):
                link = urljoin(self.BASE_URL, link)
            
            # êµ­ê°€ ì¶”ì¶œ: <a class="badge small te-stream-country">
            country_elem = element.find('a', class_=re.compile(r'te-stream-country', re.I))
            country = country_elem.get_text(strip=True) if country_elem else None
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ: <a class="badge small te-stream-category">
            category_elem = element.find('a', class_=re.compile(r'te-stream-category', re.I))
            category = category_elem.get_text(strip=True) if category_elem else None
            
            # ë³¸ë¬¸ ì¶”ì¶œ: <span class="te-stream-item-description">
            description_elem = element.find('span', class_=re.compile(r'te-stream-item-description', re.I))
            description = description_elem.get_text(strip=True) if description_elem else None
            
            # ë‚ ì§œ ì¶”ì¶œ: <small class="te-stream-item-date">
            date_elem = element.find('small', class_=re.compile(r'te-stream-item-date', re.I))
            date_text = date_elem.get_text(strip=True) if date_elem else None
            
            # ë‚ ì§œ íŒŒì‹±
            published_at = None
            if date_text:
                published_at = self._parse_date_text(date_text)
            
            return {
                'title': title,
                'link': link,
                'country': country,
                'category': category,
                'description': description,
                'published_at': published_at,
                'date_text': date_text
            }
        except Exception as e:
            logger.debug(f"ë‰´ìŠ¤ í•­ëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_date_text(self, date_text: str) -> Optional[datetime]:
        """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜"""
        if not date_text:
            return None
        
        date_text = date_text.strip().lower()
        now = datetime.now()
        
        # "X hours ago" í˜•ì‹
        hours_match = re.search(r'(\d+)\s*hours?\s*ago', date_text)
        if hours_match:
            hours = int(hours_match.group(1))
            return now - timedelta(hours=hours)
        
        # "X minutes ago" í˜•ì‹
        minutes_match = re.search(r'(\d+)\s*minutes?\s*ago', date_text)
        if minutes_match:
            minutes = int(minutes_match.group(1))
            return now - timedelta(minutes=minutes)
        
        # "X days ago" í˜•ì‹
        days_match = re.search(r'(\d+)\s*days?\s*ago', date_text)
        if days_match:
            days = int(days_match.group(1))
            return now - timedelta(days=days)
        
        return None
    
    def filter_recent_news(self, news_items: List[Dict], hours: int = 2) -> List[Dict]:
        """2ì‹œê°„ ì´ë‚´ì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        filtered = []
        
        for item in news_items:
            published_at = item.get('published_at')
            if published_at and published_at >= cutoff_time:
                filtered.append(item)
            elif not published_at:
                logger.debug(f"ë‚ ì§œ ì •ë³´ê°€ ì—†ëŠ” ë‰´ìŠ¤ ì œì™¸: {item.get('title', 'Unknown')}")
                continue
        
        logger.info(f"{hours}ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤: {len(filtered)}ê°œ")
        return filtered
    
    def print_news_item(self, item: Dict, index: int):
        """ë‰´ìŠ¤ í•­ëª©ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print(f"[{index}] {item.get('title', 'No Title')}")
        print("-" * 80)
        
        if item.get('country'):
            print(f"êµ­ê°€: {item['country']}")
        if item.get('category'):
            print(f"ì¹´í…Œê³ ë¦¬: {item['category']}")
        if item.get('date_text'):
            print(f"ë°œí–‰ ì‹œê°„: {item['date_text']} ({item.get('published_at')})")
        if item.get('link'):
            print(f"ë§í¬: {item['link']}")
        if item.get('description'):
            desc = item['description']
            if len(desc) > 200:
                desc = desc[:200] + "..."
            print(f"ë³¸ë¬¸: {desc}")
    
    def test_collect_news(self, hours: int = 2, use_selenium: bool = False):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\n" + "=" * 80)
        print("TradingEconomics ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 80)
        
        # 1. í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        html = self.fetch_stream_page(use_selenium=use_selenium)
        if not html:
            print("âŒ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # HTML ê¸°ë³¸ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“„ HTML ì •ë³´:")
        print(f"   ì „ì²´ ê¸¸ì´: {len(html)} bytes")
        print(f"   'stream' í¬í•¨: {'stream' in html}")
        print(f"   'te-stream-item' í¬í•¨: {'te-stream-item' in html}")
        print(f"   'list-group-item' í¬í•¨: {'list-group-item' in html}")
        
        # 2. ë‰´ìŠ¤ íŒŒì‹±
        print("\n[1ë‹¨ê³„] ë‰´ìŠ¤ íŒŒì‹± ì¤‘...")
        news_items = self.parse_news_items(html)
        print(f"âœ… ì´ {len(news_items)}ê°œì˜ ë‰´ìŠ¤ í•­ëª©ì„ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤.")
        
        if not news_items:
            print("âŒ íŒŒì‹±ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 3. ìµœê·¼ ë‰´ìŠ¤ í•„í„°ë§
        print(f"\n[2ë‹¨ê³„] {hours}ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ í•„í„°ë§ ì¤‘...")
        recent_news = self.filter_recent_news(news_items, hours=hours)
        print(f"âœ… {hours}ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤: {len(recent_news)}ê°œ")
        
        # 4. ê²°ê³¼ ì¶œë ¥
        print("\n[3ë‹¨ê³„] ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ëª©ë¡:")
        print("=" * 80)
        
        if recent_news:
            for idx, item in enumerate(recent_news, 1):
                self.print_news_item(item, idx)
        else:
            print(f"âŒ {hours}ì‹œê°„ ì´ë‚´ì˜ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # 5. í†µê³„ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ìˆ˜ì§‘ í†µê³„:")
        print(f"  - ì „ì²´ íŒŒì‹±ëœ ë‰´ìŠ¤: {len(news_items)}ê°œ")
        print(f"  - {hours}ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤: {len(recent_news)}ê°œ")
        
        # êµ­ê°€ë³„ í†µê³„
        countries = {}
        for item in recent_news:
            country = item.get('country', 'Unknown')
            countries[country] = countries.get(country, 0) + 1
        
        if countries:
            print("\nêµ­ê°€ë³„ ë‰´ìŠ¤ ìˆ˜:")
            for country, count in sorted(countries.items(), key=lambda x: x[1], reverse=True):
                print(f"  - {country}: {count}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        categories = {}
        for item in recent_news:
            category = item.get('category', 'Unknown')
            categories[category] = categories.get(category, 0) + 1
        
        if categories:
            print("\nì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜:")
            for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                print(f"  - {category}: {count}ê°œ")
        
        print("=" * 80)
        print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    tester = NewsCollectorTester()
    
    # Selenium ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    print("\n" + "=" * 80)
    use_selenium = input("Seleniumì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (JavaScript ë Œë”ë§ í•„ìš”, y/n): ").lower() == 'y'
    
    # 2ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
    tester.test_collect_news(hours=2, use_selenium=use_selenium)
    
    # ì¶”ê°€ ì˜µì…˜: ì „ì²´ ë‰´ìŠ¤ í™•ì¸ (í•„í„°ë§ ì—†ì´)
    print("\n" + "=" * 80)
    response = input("ì „ì²´ ë‰´ìŠ¤ ëª©ë¡ë„ í™•ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if response.lower() == 'y':
        html = tester.fetch_stream_page()
        if html:
            news_items = tester.parse_news_items(html)
            print(f"\nì „ì²´ ë‰´ìŠ¤: {len(news_items)}ê°œ")
            for idx, item in enumerate(news_items[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥
                tester.print_news_item(item, idx)
            if len(news_items) > 10:
                print(f"\n... ì™¸ {len(news_items) - 10}ê°œ ë” ìˆìŒ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\ní…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

