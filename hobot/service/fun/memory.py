from __future__ import annotations
from dotenv import load_dotenv

load_dotenv()

# ğŸ’¬ ê¸°ì–µ ì €ì¥ì†Œ ê¸°ë°˜ LLM ë©€í‹°í„´ ì±—ë´‡ (Vector DB ì—†ì´)

from datetime import datetime
from typing import List
import openai

from datetime import datetime
from typing import TypedDict, List
import openai
import json
import os
import streamlit as st

# ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")
MEMORY_FILE = "memory_store.json"

# -----------------------------
# íƒ€ì… ì •ì˜ (Python 3.11 ìŠ¤íƒ€ì¼)
# -----------------------------
class Message(TypedDict):
    role: str
    content: str

class MemoryEntry(TypedDict):
    topic: str
    summary: str
    created_at: str

# -----------------------------
# ê¸°ì–µ ì €ì¥ì†Œ (ìš”ì•½ ê¸°ë°˜ êµ¬ì¡°, íŒŒì¼ ì €ì¥)
# -----------------------------
class MemoryStore:
    def __init__(self, file_path: str = MEMORY_FILE):
        self.file_path = file_path
        self.memory: List[MemoryEntry] = self.load()

    def load(self) -> List[MemoryEntry]:
        if os.path.exists(self.file_path):
            with open(self.file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []

    def save_to_file(self) -> None:
        with open(self.file_path, 'w', encoding='utf-8') as f:
            json.dump(self.memory, f, ensure_ascii=False, indent=2)

    def save(self, topic: str, summary: str) -> None:
        self.memory.append({
            "topic": topic,
            "summary": summary,
            "created_at": datetime.now().isoformat()
        })
        self.save_to_file()

    def recall(self, query: str) -> str:
        for mem in reversed(self.memory):
            if mem["topic"] in query or query in mem["summary"]:
                return mem["summary"]
        return ""

# -----------------------------
# GPT í˜¸ì¶œ í•¨ìˆ˜ (multi-turn ëŒ€ì‘)
# -----------------------------
def gpt(messages: List[Message], temperature: float = 0.5) -> str:
    try:
        response = openai.chat.completions.create(
            model="gpt-4",
            messages=messages,
            temperature=temperature
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[ì˜¤ë¥˜] GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}"
    
def gpt_stream(messages: List[Message], temperature: float = 0.5):
    try:
        response = openai.chat.completions.create(
            model="gpt-4",
            messages=messages,
            temperature=temperature,
            stream=True  # âœ… ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        )
        full_response = ""
        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                yield content  # âœ… í•œ í† í°ì”© yield
                full_response += content
        return full_response
    except Exception as e:
        yield f"[ì˜¤ë¥˜] GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}"

# -----------------------------
# ë¬¸ì¥ ìš”ì•½ í•¨ìˆ˜
# -----------------------------
def summarize(text: str) -> str:
    prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n{text}"
    return gpt([{"role": "user", "content": prompt}], temperature=0.3)

# -----------------------------
# Streamlit UI
# -----------------------------
memory = MemoryStore()
if "message_history" not in st.session_state:
    st.session_state.message_history: List[Message] = []

st.title("ğŸ§  ê¸°ì–µ ì €ì¥ì†Œ ê¸°ë°˜ ë©€í‹°í„´ ì±—ë´‡")

memory = MemoryStore()
if "message_history" not in st.session_state:
    st.session_state.message_history: List[Message] = []

with st.form("chat_form", clear_on_submit=True):  # ğŸ‘ˆ clear_on_submit ì˜µì…˜ ì‚¬ìš©
    user_input = st.text_input("ğŸ‘¤ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", key="user_input_form")
    submitted = st.form_submit_button("ë³´ë‚´ê¸°")

if submitted and user_input:
    if "ê¸°ì–µí•´ì¤˜" in user_input:
        content = user_input.replace("ê¸°ì–µí•´ì¤˜", "").strip()
        if content:
            topic = gpt([{"role": "user", "content": f"ì´ ë¬¸ì¥ì˜ ì£¼ì œë¥¼ í•œ ë‹¨ì–´ë¡œ ì¶”ì¶œí•´ì¤˜: {content}"}], temperature=0.2)
            summary = summarize(content)
            memory.save(topic, summary)
            st.success(f"ê¸°ì–µ ì €ì¥ ì™„ë£Œ! (ì£¼ì œ: {topic})")
    else:
        context = memory.recall(user_input)
        if context:
            st.session_state.message_history.append({"role": "system", "content": f"ì°¸ê³  ì •ë³´: {context}"})

        st.session_state.message_history.append({"role": "user", "content": user_input})
        st.session_state.message_history = st.session_state.message_history[-20:]

        # âœ… ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ìˆ˜ì§‘ë§Œ, ë°”ë¡œ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
        assistant_response = ""
        for chunk in gpt_stream(st.session_state.message_history):
            assistant_response += chunk

        # âœ… ì‘ë‹µ ê¸°ë¡ë§Œ ì¶”ê°€í•˜ê³  ì¶œë ¥ì€ ì•„ë˜ ë£¨í”„ì—ì„œ
        st.session_state.message_history.append({"role": "assistant", "content": assistant_response})

# ëŒ€í™” ì¶œë ¥
for msg in st.session_state.message_history:
    role_icon = "ğŸ™‚" if msg["role"] == "user" else ("ğŸ“Œ" if msg["role"] == "system" else "ğŸ¤–")
    st.markdown(f"**{role_icon} {msg['role']}**: {msg['content']}")