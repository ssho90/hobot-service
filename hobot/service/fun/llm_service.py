from __future__ import annotations
from dotenv import load_dotenv
from datetime import datetime
from typing import List, TypedDict, Union, Literal, Any # Union, Literal, Any ì¶”ê°€
import openai
import google.generativeai as genai
import streamlit as st
import json
import os
import io # ì´ë¯¸ì§€ ì²˜ë¦¬ìš©
import base64 # Base64 ì¸ì½”ë”©ìš©
from PIL import Image # Pillow ë¼ì´ë¸ŒëŸ¬ë¦¬ (Gemini ì´ë¯¸ì§€ ì²˜ë¦¬ìš©)

# -----------------------------
# í™˜ê²½ ì„¤ì •
# -----------------------------
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
MEMORY_FILE = "memory_store.json"

# -----------------------------
# íƒ€ì… ì •ì˜ (ë©€í‹°ëª¨ë‹¬ ì§€ì›ì„ ìœ„í•´ í™•ì¥)
# -----------------------------
# OpenAI APIì˜ content í˜•ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ ì •ì˜
class TextContentPart(TypedDict):
    type: Literal["text"]
    text: str

class ImageURLContentPart(TypedDict):
    type: Literal["image_url"]
    image_url: dict # {"url": str, "detail": str} (detailì€ 'high', 'low', 'auto')

# ëŒ€í™” ê¸°ë¡ì— ì €ì¥ë  ë©”ì‹œì§€ì˜ content í˜•ì‹
# ì¼ë°˜ ë¬¸ìì—´ì´ê±°ë‚˜, í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ
ChatContent = Union[str, List[Union[TextContentPart, ImageURLContentPart]]]

class ChatMessage(TypedDict):
    role: str
    content: ChatContent

class MemoryEntry(TypedDict):
    topic: str
    summary: str
    created_at: str

# -----------------------------
# ê¸°ì–µ ì €ì¥ì†Œ (ìš”ì•½ ê¸°ë°˜ êµ¬ì¡°, SQLite ì €ì¥)
# (ì´ë¯¸ì§€ ì…ë ¥ ê¸°ëŠ¥ì€ ê¸°ì–µ ì €ì¥ì†Œì— ì§ì ‘ ì—°ê²°ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë§Œ ì €ì¥í•©ë‹ˆë‹¤.)
# -----------------------------
from service.database.memory_db import MemoryStore as SQLiteMemoryStore
from service.llm_monitoring import track_llm_call

# SQLite ê¸°ë°˜ MemoryStore ì‚¬ìš©
class MemoryStore(SQLiteMemoryStore):
    def __init__(self, file_path: str = None):
        # file_pathëŠ” í˜¸í™˜ì„±ì„ ìœ„í•´ ë°›ì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        super().__init__()

# -----------------------------
# LLM í˜¸ì¶œì„ ìœ„í•œ ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜ ìœ í‹¸ë¦¬í‹°
# -----------------------------
def _prepare_llm_messages(messages: List[ChatMessage]) -> List[Any]:
    """
    ë‚´ë¶€ ChatMessage í˜•ì‹ì„ OpenAI ë˜ëŠ” Gemini APIê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    prepared_messages = []
    for msg in messages:
        if st.session_state.llm_provider == "ChatGPT":
            # OpenAIëŠ” contentê°€ ë¬¸ìì—´ì´ê±°ë‚˜, [{type: 'text'}, {type: 'image_url'}] ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
            if isinstance(msg['content'], str):
                prepared_messages.append({"role": msg['role'], "content": msg['content']})
            else: # List[Union[TextContentPart, ImageURLContentPart]]
                prepared_messages.append({"role": msg['role'], "content": msg['content']})
        elif st.session_state.llm_provider == "Gemini":
            # GeminiëŠ” messages ë¦¬ìŠ¤íŠ¸ì˜ ê° ìš”ì†Œê°€ {"role": "user", "parts": [text_string, Image_object]} í˜•íƒœ
            gemini_parts = []
            if isinstance(msg['content'], str):
                gemini_parts.append(msg['content'])
            else: # List[Union[TextContentPart, ImageURLContentPart]]
                for part in msg['content']:
                    if part['type'] == "text":
                        gemini_parts.append(part['text'])
                    elif part['type'] == "image_url" and part['image_url']['url'].startswith("data:"):
                        # Base64 ë””ì½”ë”©í•˜ì—¬ PIL Image ê°ì²´ë¡œ ë³€í™˜
                        # data:image/jpeg;base64,.... ì—ì„œ "data:image/jpeg;base64," ë¶€ë¶„ì„ ì œê±°
                        _, encoded_image = part['image_url']['url'].split(",", 1)
                        decoded_image = base64.b64decode(encoded_image)
                        gemini_parts.append(Image.open(io.BytesIO(decoded_image)))

            # GeminiëŠ” roleì´ 'user'ì™€ 'model'ë§Œ ê°€ëŠ¥í•˜ë©°, assistantëŠ” 'model'ë¡œ ë§¤í•‘
            role_map = {"user": "user", "assistant": "model", "system": "user"} # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë„ ì‚¬ìš©ì ë©”ì‹œì§€ì²˜ëŸ¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ (Gemini Chat APIì˜ í•œê³„)
            prepared_messages.append({"role": role_map.get(msg['role'], "user"), "parts": gemini_parts})
    return prepared_messages

# -----------------------------
# GPT ë˜ëŠ” Gemini í˜¸ì¶œ í•¨ìˆ˜ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)
# -----------------------------
def gpt(messages: List[ChatMessage], temperature: float = 0.5) -> str:
    try:
        prepared_messages = _prepare_llm_messages(messages)
        
        # í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ (ì „ì²´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ)
        request_prompt_parts = []
        for msg in messages:
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            if isinstance(content, str):
                request_prompt_parts.append(f"{role}: {content}")
            elif isinstance(content, list):
                # ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ì¸ ê²½ìš° í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                text_parts = [part.get('text', '') for part in content if part.get('type') == 'text']
                if text_parts:
                    request_prompt_parts.append(f"{role}: {' '.join(text_parts)}")
        request_prompt = "\n".join(request_prompt_parts)

        # ëª¨ë¸ëª…ê³¼ ì œê³µì ê°€ì ¸ì˜¤ê¸°
        model_name = st.session_state.get('llm_model', 'unknown')
        provider = st.session_state.get('llm_provider', 'unknown')
        
        # LLM í˜¸ì¶œ ì¶”ì 
        with track_llm_call(
            model_name=model_name,
            provider=provider,
            service_name="llm_service",
            request_prompt=request_prompt
        ) as tracker:
            if st.session_state.llm_provider == "ChatGPT":
                # OpenAIëŠ” content í•„ë“œì— ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë°›ìŒ
                response = openai.chat.completions.create(
                    model=st.session_state.llm_model,
                    messages=prepared_messages, # _prepare_llm_messagesì—ì„œ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë¨
                    temperature=temperature
                )
                result = response.choices[0].message.content.strip()
                
                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ (OpenAI ì‘ë‹µì— í¬í•¨ë¨)
                if hasattr(response, 'usage'):
                    tracker.set_token_usage(
                        prompt_tokens=response.usage.prompt_tokens if hasattr(response.usage, 'prompt_tokens') else 0,
                        completion_tokens=response.usage.completion_tokens if hasattr(response.usage, 'completion_tokens') else 0,
                        total_tokens=response.usage.total_tokens if hasattr(response.usage, 'total_tokens') else 0
                    )
                
                tracker.set_response(result)
                return result
            elif st.session_state.llm_provider == "Gemini":
                # GeminiëŠ” generate_content í•¨ìˆ˜ì— List[Parts] í˜•íƒœë¡œ ë©”ì‹œì§€ë¥¼ ì „ë‹¬
                # prepared_messagesëŠ” ì´ë¯¸ Gemini Chat APIì˜ í˜•ì‹ì— ë§ê²Œ ë³€í™˜ë˜ì–´ ìˆìŒ
                model = genai.GenerativeModel(st.session_state.llm_model)

                # Geminiì˜ chat ê¸°ëŠ¥ì€ 'send_message'ë¥¼ í†µí•´ ì´ë£¨ì–´ì§€ë¯€ë¡œ,
                # chat sessionì„ ìƒì„±í•˜ê³  ê¸°ì¡´ ë©”ì‹œì§€ë“¤ì„ ì¶”ê°€í•œ ë’¤ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
                chat_session = model.start_chat(history=prepared_messages[:-1])
                response = chat_session.send_message(prepared_messages[-1]['parts'])
                result = response.text.strip()
                
                # GeminiëŠ” í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ë³„ë„ë¡œ ì œê³µí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                # ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì¶”ê°€
                if hasattr(response, 'usage_metadata'):
                    tracker.set_token_usage(
                        prompt_tokens=response.usage_metadata.prompt_token_count if hasattr(response.usage_metadata, 'prompt_token_count') else 0,
                        completion_tokens=response.usage_metadata.candidates_token_count if hasattr(response.usage_metadata, 'candidates_token_count') else 0,
                        total_tokens=response.usage_metadata.total_token_count if hasattr(response.usage_metadata, 'total_token_count') else 0
                    )
                
                tracker.set_response(result)
                return result
    except Exception as e:
        return f"[ì˜¤ë¥˜] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}"

def gpt_stream(messages: List[ChatMessage], temperature: float = 0.5):
    try:
        prepared_messages = _prepare_llm_messages(messages)
        
        # í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ (ì „ì²´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ)
        request_prompt_parts = []
        for msg in messages:
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            if isinstance(content, str):
                request_prompt_parts.append(f"{role}: {content}")
            elif isinstance(content, list):
                # ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ì¸ ê²½ìš° í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                text_parts = [part.get('text', '') for part in content if part.get('type') == 'text']
                if text_parts:
                    request_prompt_parts.append(f"{role}: {' '.join(text_parts)}")
        request_prompt = "\n".join(request_prompt_parts)

        # ëª¨ë¸ëª…ê³¼ ì œê³µì ê°€ì ¸ì˜¤ê¸°
        model_name = st.session_state.get('llm_model', 'unknown')
        provider = st.session_state.get('llm_provider', 'unknown')
        
        # LLM í˜¸ì¶œ ì¶”ì 
        with track_llm_call(
            model_name=model_name,
            provider=provider,
            service_name="llm_service_stream",
            request_prompt=request_prompt
        ) as tracker:
            if st.session_state.llm_provider == "ChatGPT":
                response = openai.chat.completions.create(
                    model=st.session_state.llm_model,
                    messages=prepared_messages, # _prepare_llm_messagesì—ì„œ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë¨
                    temperature=temperature,
                    stream=True
                )
                full_response = ""
                for chunk in response:
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        yield content
                        full_response += content
                
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì‘ë‹µ ì„¤ì •
                tracker.set_response(full_response)
                return full_response
            elif st.session_state.llm_provider == "Gemini":
                # GeminiëŠ” í˜„ì¬ ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì› (send_messageëŠ” ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹˜) â†’ ì „ì²´ ì‘ë‹µ ë°˜í™˜
                # í•˜ì§€ë§Œ gemini-1.5-pro, gemini-1.5-flash ë“±ì€ `stream=True` íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•  ìˆ˜ ìˆìœ¼ë‹ˆ
                # ì—¬ê¸°ì— í•´ë‹¹ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
                # ì´ ì½”ë“œì—ì„œëŠ” í¸ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì „ì²´ ì‘ë‹µ ë°˜í™˜
                # (ì‹¤ì œ APIëŠ” .generate_content(..., stream=True) ì§€ì›)
                model = genai.GenerativeModel(st.session_state.llm_model)
                chat_session = model.start_chat(history=prepared_messages[:-1])

                # Geminiì˜ ìŠ¤íŠ¸ë¦¬ë°ì€ `response.parts`ë¥¼ ìˆœíšŒí•˜ëŠ” ë°©ì‹
                full_response = ""
                for chunk in chat_session.send_message(prepared_messages[-1]['parts'], stream=True):
                    if chunk.text: # í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬, ì´ë¯¸ì§€ë‚˜ ë‹¤ë¥¸ íŒŒíŠ¸ ë¬´ì‹œ
                        yield chunk.text
                        full_response += chunk.text
                
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì‘ë‹µ ì„¤ì •
                tracker.set_response(full_response)
                return full_response

    except Exception as e:
        error_msg = f"[ì˜¤ë¥˜] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}"
        yield error_msg

# -----------------------------
# ë¬¸ì¥ ìš”ì•½ í•¨ìˆ˜ (í…ìŠ¤íŠ¸ë§Œ ìš”ì•½)
# -----------------------------
def summarize(text: str) -> str:
    prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n{text}"
    # summarize í•¨ìˆ˜ëŠ” í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ, ChatMessage ëŒ€ì‹  Message(str content)ë¥¼ ì‚¬ìš©
    return gpt([{"role": "user", "content": prompt}], temperature=0.3)

# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="ê¸°ì–µ ì €ì¥ ì±—ë´‡", page_icon="ğŸ§ ")
st.title("ğŸ§  ê¸°ì–µ ì €ì¥ì†Œ ê¸°ë°˜ ë©€í‹°í„´ ì±—ë´‡ (ì´ë¯¸ì§€ ì…ë ¥ ì§€ì›)")

# LLM ì„ íƒ UI
st.sidebar.title("ğŸ”§ ì„¤ì •")
llm_provider = st.sidebar.radio("LLM ì œê³µì", ["ChatGPT", "Gemini"], key="llm_provider")

if llm_provider == "ChatGPT":
    model = st.sidebar.selectbox("ChatGPT ëª¨ë¸ ì„ íƒ", ["gpt-3.5-turbo", "GPT-4o mini", "GPT-4o"], key="llm_model")
else:
    model = st.sidebar.selectbox("Gemini ëª¨ë¸ ì„ íƒ", ["gemini-2.5-pro-preview-05-06", "gemini-2.5-flash-preview-05-20", "gemini-2.0-flash", "gemini-2.0-flash-lite"], key="llm_model")

# ì‚¬ì´ë“œë°” ì„¤ì •
textarea_rows = st.sidebar.slider("ì…ë ¥ì°½ ë†’ì´ (ì¤„ ìˆ˜)", min_value=2, max_value=20, value=2)

# ê¸°ì–µ ì €ì¥ì†Œ
memory = MemoryStore()

# ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”
if "message_history" not in st.session_state:
    st.session_state.message_history: List[ChatMessage] = []

# ì…ë ¥ í¼
with st.form("chat_form", clear_on_submit=True):
    user_input = st.text_area("ğŸ‘¤ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", height=textarea_rows * 34, key="user_input_form")
    # ì´ë¯¸ì§€ ì—…ë¡œë“œ ìœ„ì ¯ ì¶”ê°€
    uploaded_file = st.file_uploader("ğŸ–¼ï¸ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”:", type=["png", "jpg", "jpeg", "webp"], key="image_uploader")
    submitted = st.form_submit_button("ë³´ë‚´ê¸°")

# ë©”ì‹œì§€ ì²˜ë¦¬
if submitted:
    # í…ìŠ¤íŠ¸ ì…ë ¥ë„ ì—†ê³ , ì´ë¯¸ì§€ ì…ë ¥ë„ ì—†ìœ¼ë©´ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
    if not user_input and not uploaded_file:
        st.warning("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.stop() # ë” ì´ìƒ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ

    # # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í˜¸í™˜ì„± ì²´í¬
    # if uploaded_file:
    #     is_multimodal_model_selected = False
    #     if st.session_state.llm_provider == "ChatGPT":
    #         if st.session_state.llm_model in ["gpt-3.5-turbo", "GPT-4o mini", "GPT-4o"]:
    #             is_multimodal_model_selected = True
    #     elif st.session_state.llm_provider == "Gemini":
    #         if st.session_state.llm_model in ["gemini-2.5-pro-preview-05-06", "gemini-2.5-flash-preview-05-20", "gemini-2.0-flash", "gemini-2.0-flash-lite"]:
    #             is_multimodal_model_selected = True

    #     if not is_multimodal_model_selected:
    #         st.warning(f"âš ï¸ ì„ íƒëœ ëª¨ë¸({st.session_state.llm_model})ì€ ì´ë¯¸ì§€ ì…ë ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ì™€ ëŒ€í™”í•˜ë ¤ë©´ GPT-4o, GPT-4o mini, ë˜ëŠ” Gemini 1.5 Pro/Flash ê°™ì€ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    #         # ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ì—ˆìœ¼ë‚˜ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¼ ê²½ìš°, ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•˜ê±°ë‚˜ ê²½ê³  í›„ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŒ.
    #         # ì—¬ê¸°ì„œëŠ” ê²½ê³ ë§Œ í•˜ê³  ì§„í–‰ì€ í•˜ë˜, LLMì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.
    #         # ë” ì•ˆì „í•œ ë°©ë²•ì€ ì—¬ê¸°ì„œ `st.stop()`ì„ í˜¸ì¶œí•˜ê±°ë‚˜ `uploaded_file = None`ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒ.
    #         # ì˜ˆë¥¼ ë“¤ì–´, `uploaded_file = None`ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ LLMìœ¼ë¡œ ì „ë‹¬ë¨.
    #         # uploaded_file = None # ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ê°•ì œ
    #         pass # ì¼ë‹¨ ê²½ê³ ë§Œ í•˜ê³  ì§„í–‰

    # ì‚¬ìš©ìì˜ í˜„ì¬ ì…ë ¥ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)ì„ ìœ„í•œ content ë¦¬ìŠ¤íŠ¸ ìƒì„±
    current_user_content: List[Union[TextContentPart, ImageURLContentPart]] = []

    if user_input:
        # "ê¸°ì–µí•´" í‚¤ì›Œë“œ ì²˜ë¦¬ (ì´ë¯¸ì§€ ê¸°ì–µì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŒ)
        if "ê¸°ì–µí•´ì¤˜" in user_input:
            content_to_memorize = user_input.replace("ê¸°ì–µí•´ì¤˜", "").strip()
            if content_to_memorize:
                # ê¸°ì–µí•  ë‚´ìš©ì´ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ í…ìŠ¤íŠ¸ë§Œ summarizeì— ì „ë‹¬
                topic = gpt([{"role": "user", "content": f"ì´ ë¬¸ì¥ì˜ ì£¼ì œë¥¼ í•œ ë‹¨ì–´ë¡œ ì¶”ì¶œí•´ì¤˜: {content_to_memorize}"}], temperature=0.2)
                summary = summarize(content_to_memorize)
                memory.save(topic, summary)
                st.success(f"ê¸°ì–µ ì €ì¥ ì™„ë£Œ! (ì£¼ì œ: {topic})")
                # ê¸°ì–µ ì €ì¥ì´ ì™„ë£Œëœ ê²½ìš°, LLM í˜¸ì¶œ ì—†ì´ ëŒ€í™” ì¢…ë£Œ
                # ì´ ê²½ìš° ì±—ë´‡ì€ ì‘ë‹µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì‚¬ìš©ìì—ê²Œ í”¼ë“œë°±ì„ ì£¼ëŠ” ê²ƒì´ ì¢‹ìŒ
                st.session_state.message_history.append({"role": "system", "content": f"âœ… 'ê¸°ì–µí•´ì¤˜: {content_to_memorize[:30]}...' ì €ì¥ ì™„ë£Œ!"})
                st.experimental_rerun() # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ë‹¤ì‹œ ë¡œë“œ
            else:
                st.warning("ë¬´ì—‡ì„ ê¸°ì–µí•´ì•¼ í• ì§€ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: 'ê¸°ì–µí•´ ì ì‹¬ì€ ëœì¥ì°Œê°œ'")
            st.stop() # 'ê¸°ì–µí•´ì¤˜' ì²˜ë¦¬ëŠ” LLM í˜¸ì¶œ ì—†ì´ ì¢…ë£Œ

        current_user_content.append({"type": "text", "text": user_input})

    if uploaded_file:
        # íŒŒì¼ ë‚´ìš©ì„ Base64ë¡œ ì¸ì½”ë”©
        bytes_data = uploaded_file.getvalue()
        base64_image = base64.b64encode(bytes_data).decode("utf-8")
        mime_type = uploaded_file.type # ì˜ˆ: 'image/jpeg'

        current_user_content.append({
            "type": "image_url",
            "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}
            # "detail": "high" ë˜ëŠ” "low"ë¥¼ ì¶”ê°€í•˜ì—¬ ì´ë¯¸ì§€ í’ˆì§ˆ ì„¤ì • ê°€ëŠ¥ (OpenAI)
        })

    # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    # current_user_contentëŠ” List[Union[TextContentPart, ImageURLContentPart]] ì´ë¯€ë¡œ ChatContent íƒ€ì…ì— ë§ìŒ
    st.session_state.message_history.append({"role": "user", "content": current_user_content})
    st.session_state.message_history = st.session_state.message_history[-20:] # ìµœê·¼ 20ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€

    # ê¸°ì–µ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° (í…ìŠ¤íŠ¸ ì…ë ¥ ê¸°ë°˜)
    context = memory.recall(user_input if user_input else "") # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ì „ë‹¬
    if context:
        st.session_state.message_history.append({"role": "system", "content": f"ì°¸ê³  ì •ë³´: {context}"})

    # LLMìœ¼ë¡œë¶€í„° ì‘ë‹µ ë°›ê¸°
    assistant_response_content = ""
    for chunk in gpt_stream(st.session_state.message_history):
        assistant_response_content += chunk
    st.session_state.message_history.append({"role": "assistant", "content": assistant_response_content})

# ëŒ€í™” ì¶œë ¥
for msg in st.session_state.message_history:
    role_icon = "ğŸ™‚" if msg["role"] == "user" else ("ğŸ“Œ" if msg["role"] == "system" else "ğŸ¤–")

    st.markdown(f"**{role_icon} {msg['role']}**:")

    # contentê°€ ë¬¸ìì—´ì¸ ê²½ìš° (ê¸°ì¡´ í…ìŠ¤íŠ¸ ë©”ì‹œì§€, ì‹œìŠ¤í…œ ë©”ì‹œì§€)
    if isinstance(msg['content'], str):
        st.markdown(msg['content'])
    # contentê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€)
    else:
        for part in msg['content']:
            if part['type'] == "text":
                st.markdown(part['text'])
            elif part['type'] == "image_url":
                try:
                    # Base64 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë””ì½”ë”©í•˜ì—¬ í‘œì‹œ
                    # 'data:image/jpeg;base64,' ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ì‹¤ì œ Base64 ë°ì´í„°ë§Œ ì¶”ì¶œ
                    _, encoded_image = part['image_url']['url'].split(",", 1)
                    st.image(base64.b64decode(encoded_image), use_container_width =True)
                except Exception as e:
                    st.error(f"ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")